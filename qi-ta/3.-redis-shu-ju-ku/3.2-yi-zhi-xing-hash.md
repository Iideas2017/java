# 3.2 一致性hash

## 1. 使用背景

         **一致性哈希算法**在1997年由麻省理工学院的Karger等人在**解决分布式Cache**中提出的，设计目标是为了解决因特网中的**热点\(Hot spot\)**问题，初衷和CARP十分类似。一致性哈希修正了CARP使用的简单哈希算法带来的问题，使得DHT可以在P2P环境中真正得到应用。

        比如你有 N 个 cache 服务器（后面简称 cache ），那么如何将一个对象 object 映射到 N 个 cache 上呢，你很可能会采用类似下面的通用方法计算 object 的 hash 值，然后均匀的映射到到 N 个 cache ；

> **求余算法: hash\(object\)%N**

一切都运行正常，再考虑如下的**两种情况**：  
 1 一个 cache 服务器 m down 掉了（在实际应用中必须要考虑这种情况），这样所有映射到 cache m 的对象都会失效，怎么办，需要把 cache m 从 cache 中移除，这时候 cache 是 N-1 台，映射公式变成了 **hash\(object\)%\(N-1\)** ；  
 2 由于访问加重，需要添加 cache ，这时候 cache 是 N+1 台，映射公式变成了**hash\(object\)%\(N+1\)** ；

## 2. 良好的hash算法

良好的分布式cahce系统中的一致性hash算法应该满足以下几个方面：

### **2.1 平衡性\(Balance\)**

     平衡性是指哈希的结果能够尽可能分布到所有的缓冲中去，这样可以使得所有的缓冲空间都得到利用。很多哈希算法都能够满足这一条件。

### **2.2 单调性\(Monotonicity\)**

     单调性是指如果已经有一些内容通过哈希分派到了相应的缓冲中，又有新的缓冲区加入到系统中，那么哈希的结果应能够保证原有已分配的内容可以被映射到新的缓冲区中去，而不会被映射到旧的缓冲集合中的其他缓冲区。简单的哈希算法往往不能满足单调性的要求，如最简单的线性哈希：x = \(ax + b\) mod \(P\)，在上式中，P表示全部缓冲的大小。不难看出，当缓冲大小发生变化时\(从P1到P2\)，原来所有的哈希结果均会发生变化，从而不满足单调性的要求。哈希结果的变化意味着当缓冲空间发生变化时，所有的映射关系需要在系统内全部更新。而在P2P系统内，缓冲的变化等价于Peer加入或退出系统，这一情况在P2P系统中会频繁发生，因此会带来极大计算和传输负荷。单调性就是要求哈希算法能够应对这种情况。

### **2.3 分散性\(Spread\)**

         在分布式环境中，终端有可能看不到所有的缓冲，而是只能看到其中的一部分。当终端希望通过哈希过程将内容映射到缓冲上时，由于不同终端所见的缓冲范围有可能不同，从而导致哈希的结果不一致，最终的结果是相同的内容被不同的终端映射到不同的缓冲区中。这种情况显然是应该避免的，因为它导致相同内容被存储到不同缓冲中去，降低了系统存储的效率。分散性的定义就是上述情况发生的严重程度。好的哈希算法应能够尽量避免不一致的情况发生，也就是尽量降低分散性。

### **2.4 负载\(Load\)**

        负载问题实际上是从另一个角度看待分散性问题。既然不同的终端可能将相同的内容映射到不同的缓冲区中，那么对于一个特定的缓冲区而言，也可能被不同的用户映射为不同的内容。与分散性一样，这种情况也是应当避免的，因此好的哈希算法应能够尽量降低缓冲的负荷。

### **2.5 平滑性\(Smoothness\)**

      平滑性是指缓存服务器的数目平滑改变和缓存对象的平滑改变是一致的。

## 3.一致性hash原理

### **3.1 虚拟节点**

考量 Hash 算法的另一个指标是**平衡性 \(**Balance\) ，  
        hash 算法并不是保证绝对的平衡，如果 cache 较少的话，对象并不能被均匀的映射到 cache 上,分布是很不均衡的。  
       即对每一个服务节点计算多个哈希，每个计算结果位置都放置一个此服务节点，称为虚拟节点。具体做法可以在服务器ip或主机名的后面增加编号来实现。例如上面的情况，可以为每台服务器计算三个虚拟节点，于是可以分别计算 “Node A\#1”、“Node A\#2”、“Node A\#3”、“Node B\#1”、“Node B\#2”、“Node B\#3”的哈希值，于是形成六个虚拟节点：

![](../../.gitbook/assets/image%20%2865%29.png)

 同时数据定位算法不变，只是多了一步虚拟节点到实际节点的映射，例如定位到“Node A\#1”、“Node A\#2”、“Node A\#3”三个虚拟节点的数据均定位到Node A上。这样就解决了服务节点少时数据倾斜的问题。在实际应用中，通常将虚拟节点数设置为32甚至更大，因此即使很少的服务节点也能做到相对均匀的数据分布。

![](../../.gitbook/assets/image%20%28378%29.png)

引入“虚拟节点”前，计算 cache A 的 hash 值：  
 Hash\(“202.168.14.241”\);  
 引入“虚拟节点”后，计算“虚拟节”点 cache A1 和 cache A2 的 hash 值：  
 Hash\(“202.168.14.241\#1”\); // cache A1  
 Hash\(“202.168.14.241\#2”\); // cache A2



